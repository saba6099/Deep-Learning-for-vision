{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os, struct\n",
    "from array import array as pyarray\n",
    "from numpy import  array, zeros\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for reading the MNIST Dataset\n",
    "def load_mnist(dataset=\"training\", digits=np.arange(10), path=\".\"):\n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    flbl = open(fname_lbl, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = pyarray(\"b\", flbl.read())\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = open(fname_img, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
    "    N = len(ind)\n",
    "\n",
    "    images = zeros((N, rows, cols) )\n",
    "    labels = zeros((N ) )\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saba/.local/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read in training and test data\n",
    "X_train, y_train = load_mnist('training')\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_train = np.divide(X_train, 256)\n",
    "X_test, y_test = load_mnist('testing')\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_test = np.divide(X_test, 256)\n",
    "\n",
    "# one hot encoding for train labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded_train_y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# Starting values for weights W and bias b\n",
    "# np.random.seed(1)\n",
    "b = np.zeros(10)\n",
    "W = 2*np.random.random((X_train.shape[1],10)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    a = a - np.max(a)\n",
    "    exp = np.exp(a)\n",
    "    sum_exp = np.sum(exp)\n",
    "    logits = np.divide(exp, sum_exp)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(X,W,b):\n",
    "    return softmax(np.dot(X,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing gradients and loss\n",
    "def l2loss(X,y,W,b):\n",
    "    pred_y = predict(X, W, b)\n",
    "    l2 = np.sum(np.square(y-pred_y))\n",
    "    gradE_pred = y - pred_y\n",
    "    gradE_h = gradE_pred*pred_y*(1-pred_y)\n",
    "    gradE_W  = np.multiply(-2, np.dot(X.T, gradE_h))\n",
    "    gradE_b = np.mean(-2*gradE_h, axis=0)\n",
    "    return l2, gradE_W, gradE_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the data for given iterations and learning rate\n",
    "def train(X,y,W,b, num_iters=10, eta=0.001):\n",
    "    \"\"\"\n",
    "    X: N-by-D array of training data\n",
    "    y: N dimensional numpy array of labels\n",
    "    W: D dimensional array of weights\n",
    "    b: scalar bias\n",
    "    num_iters: (optional) number of steps to take when optimizing\n",
    "    eta: (optional)  the stepsize for the gradient descent\n",
    "\n",
    "    Should return the final values of W and b\n",
    "     \"\"\"\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        l, gradE_W, gradE_b = l2loss(X, y, W, b)\n",
    "        # gradE_W = gradE_W.reshape((gradE_W.shape[0], 1))\n",
    "        # gradE_b = gradE_b.reshape((gradE_b.shape[0], 1))\n",
    "        W = np.subtract(W, eta*gradE_W)\n",
    "        b = np.subtract(b, eta*gradE_b)\n",
    "        loss[i] = loss[i] + l\n",
    "        \n",
    "    return W, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the data with batch size as 20\n",
    "num_iters = 600\n",
    "eta = 0.005\n",
    "loss = np.zeros(num_iters)\n",
    "batch_size = 20\n",
    "start_index = 0\n",
    "for i in range(int(len(X_train)/batch_size)):\n",
    "    end_index = start_index + batch_size\n",
    "    W,b = train(X_train[start_index: end_index], onehot_encoded_train_y[start_index: end_index], W, b, num_iters, eta)\n",
    "    start_index = start_index + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of loss vs iterations\n",
    "%matplotlib inline\n",
    "plt.plot(range(num_iters), loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for test data\n",
    "pred = predict(X_test, W, b)\n",
    "# converting softmax predicted values to labels\n",
    "yhat = np.argmax(pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(yhat==y_test)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
