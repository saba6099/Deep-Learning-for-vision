{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR Classification with basic NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saba6099/Deep-Learning-for-vision/blob/master/CIFAR_Classification_with_basic_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmity0MqsVsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import random as rand\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt \n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l52F4Uaksbgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0cu3E1ZHdIr",
        "colab_type": "code",
        "outputId": "5bf0bc85-d0fb-4b51-daee-b157aa1bc44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 168747008/170498071 [00:12<00:00, 16984701.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMy8Zt-1Op61",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHd4Q3dLtheS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr40PMILtyKT",
        "colab_type": "code",
        "outputId": "b1e2b77a-0cad-42df-a901-551b4d691bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "CLASSES[trainset[100][1]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ship'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQJQXDFf3JA8",
        "colab_type": "code",
        "outputId": "c53d5a60-3926-419a-81f1-63e683b8154f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trainset[1][0].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbczOfnfwIsI",
        "colab_type": "code",
        "outputId": "ea0e4b9b-f49f-4d44-9c7b-b66185f75057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        " \n",
        "batch_size = 100\n",
        "n_iters = 50000\n",
        "num_epochs = n_iters / (len(trainset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "print(\"length of trainset:\", len(trainset))\n",
        "print(\"Number of epochs: \", num_epochs)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=trainset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        " \n",
        "test_loader = torch.utils.data.DataLoader(dataset=testset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of trainset: 50000\n",
            "Number of epochs:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BucS985LEbu",
        "colab_type": "code",
        "outputId": "79c8b6da-363e-4c28-ba02-8c5b83761208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IXjFI00wWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        # Linear function 1: 784 --> 100\n",
        "        self.fc1 = nn.Linear(input_dim, 500) \n",
        "        # Non-linearity 1\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # Linear function 2: 100 --> 100\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function 1\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity 1\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        # Linear function 2\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSs-JVyewn3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 32*32*3\n",
        "hidden_dim = 1500\n",
        "output_dim = 10\n",
        " \n",
        "model = LogisticRegressionModel(input_dim,hidden_dim, output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPbeWgOh9kmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUNfGLRsxf3X",
        "colab_type": "code",
        "outputId": "cfcc55f5-9a3c-4190-c8c2-c3f1c8f95a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    avDev = torch.device(\"cuda\")\n",
        "else:\n",
        "    avDev = torch.device(\"cpu\")\n",
        "\n",
        "print(avDev)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P32Rir2-xGD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  USE GPU FOR MODEL  #\n",
        "#######################\n",
        " \n",
        "model.to(avDev)\n",
        " \n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss().to(avDev)\n",
        "\n",
        " \n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.01\n",
        " \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum= 0.9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ZkVaU1qKLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training curve\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "train_summary = SummaryWriter('train_summary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ffvg9SxmVA",
        "colab_type": "code",
        "outputId": "71af8f61-519b-440c-8e9e-43a93fb06189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "loss_values = []\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "         \n",
        "        #print(images.size()) #torch.Size([100, 1, 28, 28])\n",
        "        #print(images.view(-1, 32*32*3).size()) #torch.Size([100, 784])\n",
        "        images = images.view(-1, 32*32*3).to(avDev)\n",
        "        labels = labels.to(avDev)\n",
        "        #print(labels) \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "         \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)#\n",
        "         \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "         \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "         \n",
        "        iter += 1\n",
        "         \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                #######################\n",
        "                #  USE GPU FOR MODEL  #\n",
        "                #######################\n",
        "                images = images.view(-1, 32*32*3).to(avDev)\n",
        "                 \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                 \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                 \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                 \n",
        "                #######################\n",
        "                #  USE GPU FOR MODEL  #\n",
        "                #######################\n",
        "                # Total correct predictions\n",
        "                correct += (predicted.cpu() == labels.cpu()).sum().float()\n",
        "             \n",
        "            accuracy = 100. * correct / total\n",
        "            loss_values.append(loss.item())\n",
        "            # loss = np.array(train_summary.read_scalar(\"Loss\"))\n",
        "            # plt.plot(loss[:, 0], loss[:, 1], label='train loss')\n",
        "            # plt.legend()\n",
        "            # plt.show()\n",
        "            # plt.plot(loss.item(), iter)\n",
        "            # print(\"Loss\", type(loss.item()))\n",
        "            # plt.show()\n",
        "             \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "plt.plot(loss_values, 'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 1.6001091003417969. Accuracy: 42.290000915527344\n",
            "Iteration: 1000. Loss: 1.2527391910552979. Accuracy: 44.38999938964844\n",
            "Iteration: 1500. Loss: 1.265075922012329. Accuracy: 46.099998474121094\n",
            "Iteration: 2000. Loss: 1.309683084487915. Accuracy: 46.04999923706055\n",
            "Iteration: 2500. Loss: 1.131248116493225. Accuracy: 46.81999969482422\n",
            "Iteration: 3000. Loss: 1.1907014846801758. Accuracy: 46.65999984741211\n",
            "Iteration: 3500. Loss: 1.182090163230896. Accuracy: 47.7400016784668\n",
            "Iteration: 4000. Loss: 0.9841108918190002. Accuracy: 47.470001220703125\n",
            "Iteration: 4500. Loss: 1.2116827964782715. Accuracy: 48.75\n",
            "Iteration: 5000. Loss: 1.1654516458511353. Accuracy: 49.97999954223633\n",
            "Iteration: 5500. Loss: 0.9077140688896179. Accuracy: 50.68000030517578\n",
            "Iteration: 6000. Loss: 1.0041264295578003. Accuracy: 48.869998931884766\n",
            "Iteration: 6500. Loss: 1.0187968015670776. Accuracy: 50.0099983215332\n",
            "Iteration: 7000. Loss: 0.8721417784690857. Accuracy: 51.33000183105469\n",
            "Iteration: 7500. Loss: 1.0963501930236816. Accuracy: 49.939998626708984\n",
            "Iteration: 8000. Loss: 0.8725286722183228. Accuracy: 50.939998626708984\n",
            "Iteration: 8500. Loss: 0.8968873620033264. Accuracy: 53.0\n",
            "Iteration: 9000. Loss: 0.8772310614585876. Accuracy: 51.040000915527344\n",
            "Iteration: 9500. Loss: 0.9060176014900208. Accuracy: 51.20000076293945\n",
            "Iteration: 10000. Loss: 0.8204702138900757. Accuracy: 49.79999923706055\n",
            "Iteration: 10500. Loss: 0.6301680207252502. Accuracy: 50.22999954223633\n",
            "Iteration: 11000. Loss: 0.6977717876434326. Accuracy: 50.81999969482422\n",
            "Iteration: 11500. Loss: 0.9664724469184875. Accuracy: 51.70000076293945\n",
            "Iteration: 12000. Loss: 0.7214248180389404. Accuracy: 51.91999816894531\n",
            "Iteration: 12500. Loss: 0.6766564846038818. Accuracy: 50.119998931884766\n",
            "Iteration: 13000. Loss: 0.7937912940979004. Accuracy: 51.59000015258789\n",
            "Iteration: 13500. Loss: 0.7316823601722717. Accuracy: 51.880001068115234\n",
            "Iteration: 14000. Loss: 0.6805640459060669. Accuracy: 51.349998474121094\n",
            "Iteration: 14500. Loss: 0.9121562838554382. Accuracy: 51.380001068115234\n",
            "Iteration: 15000. Loss: 0.7135636806488037. Accuracy: 51.75\n",
            "Iteration: 15500. Loss: 0.4656076431274414. Accuracy: 50.869998931884766\n",
            "Iteration: 16000. Loss: 0.730197012424469. Accuracy: 51.86000061035156\n",
            "Iteration: 16500. Loss: 0.607478678226471. Accuracy: 52.56999969482422\n",
            "Iteration: 17000. Loss: 0.6858416795730591. Accuracy: 49.93000030517578\n",
            "Iteration: 17500. Loss: 0.6972798109054565. Accuracy: 52.61000061035156\n",
            "Iteration: 18000. Loss: 0.8085051774978638. Accuracy: 51.220001220703125\n",
            "Iteration: 18500. Loss: 0.5453332662582397. Accuracy: 49.599998474121094\n",
            "Iteration: 19000. Loss: 0.7529816627502441. Accuracy: 49.70000076293945\n",
            "Iteration: 19500. Loss: 0.5611656904220581. Accuracy: 50.880001068115234\n",
            "Iteration: 20000. Loss: 0.7169145345687866. Accuracy: 52.93000030517578\n",
            "Iteration: 20500. Loss: 0.6184561252593994. Accuracy: 51.06999969482422\n",
            "Iteration: 21000. Loss: 0.5735843777656555. Accuracy: 52.18000030517578\n",
            "Iteration: 21500. Loss: 0.471510112285614. Accuracy: 52.5\n",
            "Iteration: 22000. Loss: 0.4418613314628601. Accuracy: 51.310001373291016\n",
            "Iteration: 22500. Loss: 0.5820940136909485. Accuracy: 51.189998626708984\n",
            "Iteration: 23000. Loss: 0.4961584508419037. Accuracy: 51.93000030517578\n",
            "Iteration: 23500. Loss: 0.5816066265106201. Accuracy: 50.790000915527344\n",
            "Iteration: 24000. Loss: 0.4262237548828125. Accuracy: 52.380001068115234\n",
            "Iteration: 24500. Loss: 0.3514908254146576. Accuracy: 52.33000183105469\n",
            "Iteration: 25000. Loss: 0.6631458401679993. Accuracy: 51.66999816894531\n",
            "Iteration: 25500. Loss: 0.6214419007301331. Accuracy: 51.630001068115234\n",
            "Iteration: 26000. Loss: 0.41635027527809143. Accuracy: 49.88999938964844\n",
            "Iteration: 26500. Loss: 0.4673457443714142. Accuracy: 51.0\n",
            "Iteration: 27000. Loss: 0.3107891082763672. Accuracy: 51.5099983215332\n",
            "Iteration: 27500. Loss: 0.43205153942108154. Accuracy: 52.5\n",
            "Iteration: 28000. Loss: 0.7583378553390503. Accuracy: 52.02000045776367\n",
            "Iteration: 28500. Loss: 0.5322921276092529. Accuracy: 51.959999084472656\n",
            "Iteration: 29000. Loss: 0.346649706363678. Accuracy: 51.56999969482422\n",
            "Iteration: 29500. Loss: 0.29872551560401917. Accuracy: 52.150001525878906\n",
            "Iteration: 30000. Loss: 0.3521559238433838. Accuracy: 52.970001220703125\n",
            "Iteration: 30500. Loss: 0.3879866898059845. Accuracy: 51.34000015258789\n",
            "Iteration: 31000. Loss: 0.4550994038581848. Accuracy: 51.70000076293945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qmEmqoJUCSd",
        "colab_type": "text"
      },
      "source": [
        "Getting Maximum Accuracy of 49 percent.\n",
        "Things to do:\n",
        "1) learning curve\n",
        "2) Confusion Matrix"
      ]
    }
  ]
}